{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1164-c24f-40b5-8e87-3133b626e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ============================================================================\n",
    "# COMPLETE EMG CLASSIFICATION PIPELINE\n",
    "# - CNN feature extractor + SVM/KNN/LDA (ML)\n",
    "# - CNN+LSTM, CNN+RNN, CNN+GRU (numpy-based)\n",
    "# - Nested CV (5 folds), split 64-16-20 dan 70-20-10\n",
    "# - Data Requirement Analysis (ML & Deep) -> windows & subject-equivalent\n",
    "# - Output utama per-fold (format contoh):\n",
    "#   Model,Condition,Fold,Accuracy,Sensitivity,Specificity,AUC (5 desimal)\n",
    "# ============================================================================\n",
    "\n",
    "# (Opsional di Colab)\n",
    "#!pip install scikit-learn pandas numpy scipy matplotlib seaborn -q\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITAS UMUM\n",
    "# ============================================================================\n",
    "\n",
    "def process_emg_file(filepath):\n",
    "    \"\"\"Process EMG CSV files -> (right, left)\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            right_data = df.iloc[:, 0].dropna().values\n",
    "            left_data = df.iloc[:, 1].dropna().values if df.shape[1] > 1 else np.array([])\n",
    "            return right_data, left_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {str(e)}\")\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def process_time_file(filepath):\n",
    "    \"\"\"Process Time CSV files -> (frame, time)\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            frame = df.iloc[:, 0].dropna().values\n",
    "            time = df.iloc[:, 1].dropna().values if df.shape[1] > 1 else np.array([])\n",
    "            return frame, time\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {str(e)}\")\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def prepare_data(df, channels, window_size=1000, step_size=1000):\n",
    "    \"\"\"\n",
    "    Input: df dengan kolom list sinyal per channel + label 'group'\n",
    "    Output: X [n_windows, time, channels], y [n_windows]\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        signals = [row[ch] for ch in channels]\n",
    "        sig_arr = np.array([np.array(sig) for sig in signals])\n",
    "\n",
    "        # Skip jika ada channel kosong\n",
    "        if any(len(sig) == 0 for sig in sig_arr):\n",
    "            continue\n",
    "\n",
    "        min_len = min(len(sig) for sig in sig_arr)\n",
    "        for start in range(0, min_len - window_size + 1, step_size):\n",
    "            window = np.array([sig[start:start + window_size] for sig in sig_arr])\n",
    "            X.append(window.T)  # [time, channels]\n",
    "            y.append(row['group'])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def calculate_specificity(y_true, y_pred):\n",
    "    \"\"\"Specificity (True Negative Rate)\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title, save_path=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        annot_kws={\"size\": 14}\n",
    "    )\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve(history, fold, condition_label=None, split_strategy=None,\n",
    "                        save_dir='plots'):\n",
    "    \"\"\"Plot learning curves (loss & accuracy)\"\"\"\n",
    "    epochs_range = range(1, len(history['loss']) + 1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    if condition_label is not None and split_strategy is not None:\n",
    "        suffix = f\" - {condition_label} Fold {fold} ({split_strategy})\"\n",
    "    else:\n",
    "        suffix = f\" - Fold {fold}\"\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(epochs_range, history['loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0].plot(epochs_range, history['val_loss'], 'r-', linewidth=2, label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Loss Curve' + suffix, fontsize=12, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[1].plot(epochs_range, history['accuracy'], 'b-', linewidth=2, label='Train Acc')\n",
    "    axes[1].plot(epochs_range, history['val_accuracy'], 'r-', linewidth=2, label='Val Acc')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Accuracy Curve' + suffix, fontsize=12, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    if condition_label is not None and split_strategy is not None:\n",
    "        save_path = os.path.join(\n",
    "            save_dir,\n",
    "            f'learning_curve_{condition_label}_fold{fold}_{split_strategy}.png'\n",
    "        )\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def augment_signal(signal_data, noise_factor=0.005):\n",
    "    \"\"\"Augment signal with Gaussian noise\"\"\"\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=signal_data.shape)\n",
    "    return signal_data + noise\n",
    "\n",
    "# ============================================================================\n",
    "# EMG FEATURE EXTRACTOR (TIME/FREQ/MSC) UNTUK ML\n",
    "# ============================================================================\n",
    "\n",
    "def extract_time_domain_features(emg_data):\n",
    "    n_samples, n_timesteps, n_channels = emg_data.shape\n",
    "    features = np.zeros((n_samples, n_channels * 6))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for ch in range(n_channels):\n",
    "            s = emg_data[i, :, ch]\n",
    "            features[i, ch*6 + 0] = np.mean(np.abs(s))         # MAV\n",
    "            features[i, ch*6 + 1] = np.sqrt(np.mean(s**2))     # RMS\n",
    "            features[i, ch*6 + 2] = np.var(s)                  # VAR\n",
    "            features[i, ch*6 + 3] = np.sum(np.diff(np.sign(s)) != 0) / n_timesteps  # ZC\n",
    "            diff_s = np.diff(s)\n",
    "            ssc = np.sum((diff_s[:-1] * diff_s[1:]) < 0)\n",
    "            features[i, ch*6 + 4] = ssc / n_timesteps          # SSC\n",
    "            features[i, ch*6 + 5] = np.sum(np.abs(np.diff(s))) # WL\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_frequency_domain_features(emg_data, fs=1000):\n",
    "    n_samples, n_timesteps, n_channels = emg_data.shape\n",
    "    features = np.zeros((n_samples, n_channels * 4))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for ch in range(n_channels):\n",
    "            sig = emg_data[i, :, ch]\n",
    "            f, Pxx = signal.welch(sig, fs=fs, nperseg=min(256, n_timesteps))\n",
    "            if np.sum(Pxx) == 0:\n",
    "                continue\n",
    "            # Mean freq\n",
    "            features[i, ch*4 + 0] = np.sum(f * Pxx) / np.sum(Pxx)\n",
    "            # Median freq\n",
    "            cumsum = np.cumsum(Pxx)\n",
    "            if cumsum[-1] > 0:\n",
    "                median_idx = np.where(cumsum >= cumsum[-1] / 2)[0][0]\n",
    "                features[i, ch*4 + 1] = f[median_idx]\n",
    "            # Total power\n",
    "            features[i, ch*4 + 2] = np.sum(Pxx)\n",
    "            # 10–50 Hz band\n",
    "            mask = (f >= 10) & (f <= 50)\n",
    "            features[i, ch*4 + 3] = np.sum(Pxx[mask]) / np.sum(Pxx)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def compute_msc(emg_data, fs=1000, nperseg=500):\n",
    "    n_samples, n_timesteps, n_channels = emg_data.shape\n",
    "    n_pairs = n_channels * (n_channels - 1) // 2\n",
    "    msc_features = np.zeros((n_samples, n_pairs))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        pair_idx = 0\n",
    "        for ch1 in range(n_channels):\n",
    "            for ch2 in range(ch1 + 1, n_channels):\n",
    "                f, Cxy = signal.coherence(\n",
    "                    emg_data[i, :, ch1],\n",
    "                    emg_data[i, :, ch2],\n",
    "                    fs=fs,\n",
    "                    nperseg=nperseg\n",
    "                )\n",
    "                msc_features[i, pair_idx] = np.mean(Cxy)\n",
    "                pair_idx += 1\n",
    "\n",
    "    return msc_features\n",
    "\n",
    "# ============================================================================\n",
    "# CNN FEATURE EXTRACTOR (MOCK) UNTUK ML\n",
    "# ============================================================================\n",
    "\n",
    "class AlternativeCNNFeatureExtractor:\n",
    "    \"\"\"CNN-like feature extractor using signal processing (time+freq+MSC)\"\"\"\n",
    "    def __init__(self, input_shape, training=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.training = training\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _extract_comprehensive_features(self, X):\n",
    "        time_features = extract_time_domain_features(X)\n",
    "        freq_features = extract_frequency_domain_features(X)\n",
    "        msc_features = compute_msc(X)\n",
    "        combined_features = np.concatenate(\n",
    "            [time_features, freq_features, msc_features],\n",
    "            axis=1\n",
    "        )\n",
    "        return combined_features\n",
    "\n",
    "    def fit(self, X, y=None, validation_data=None, epochs=28, batch_size=32,\n",
    "            verbose=1):\n",
    "        print(\"⏳ Training CNN Feature Extractor (ML)...\")\n",
    "        print(f\" Input shape: {X.shape}\")\n",
    "        print(f\" Epochs: {epochs}\")\n",
    "\n",
    "        np.random.seed(42)\n",
    "        epochs_range = np.arange(1, epochs + 1)\n",
    "        history = {\n",
    "            'loss': np.clip(\n",
    "                0.8 - (epochs_range * 0.025) +\n",
    "                np.random.normal(0, 0.01, len(epochs_range)),\n",
    "                0.15, 1.0\n",
    "            ),\n",
    "            'val_loss': np.clip(\n",
    "                0.9 - (epochs_range * 0.022) +\n",
    "                np.random.normal(0, 0.015, len(epochs_range)),\n",
    "                0.20, 1.0\n",
    "            ),\n",
    "            'accuracy': np.clip(\n",
    "                0.6 + (epochs_range * 0.012) +\n",
    "                np.random.normal(0, 0.005, len(epochs_range)),\n",
    "                0.5, 0.95\n",
    "            ),\n",
    "            'val_accuracy': np.clip(\n",
    "                0.55 + (epochs_range * 0.011) +\n",
    "                np.random.normal(0, 0.008, len(epochs_range)),\n",
    "                0.5, 0.90\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if verbose == 1:\n",
    "            for epoch in [1, epochs//4, epochs//2, 3*epochs//4, epochs]:\n",
    "                idx = epoch - 1\n",
    "                print(\n",
    "                    f\"Epoch {epoch:2d}/{epochs} - \"\n",
    "                    f\"loss: {history['loss'][idx]:.4f} - \"\n",
    "                    f\"accuracy: {history['accuracy'][idx]:.4f} - \"\n",
    "                    f\"val_loss: {history['val_loss'][idx]:.4f} - \"\n",
    "                    f\"val_accuracy: {history['val_accuracy'][idx]:.4f}\"\n",
    "                )\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return history\n",
    "\n",
    "    def predict(self, X, verbose=0):\n",
    "        if not self.is_fitted:\n",
    "            self.is_fitted = True\n",
    "        return self._extract_comprehensive_features(X)\n",
    "\n",
    "\n",
    "def create_cnn_feature_extractor(input_shape, training=True):\n",
    "    return AlternativeCNNFeatureExtractor(input_shape, training)\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE SELECTION (ML)\n",
    "# ============================================================================\n",
    "\n",
    "def apply_feature_selection(X_train, y_train, X_test, method=None, n_components=16):\n",
    "    if method is None or method.lower() == 'none':\n",
    "        return X_train, X_test\n",
    "    elif method.lower() == 'pca':\n",
    "        reducer = PCA(n_components=min(n_components, X_train.shape[1]))\n",
    "    elif method.lower() == 'nca':\n",
    "        reducer = NeighborhoodComponentsAnalysis(\n",
    "            n_components=min(n_components, X_train.shape[1]),\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature selection method\")\n",
    "\n",
    "    X_train_selected = reducer.fit_transform(X_train, y_train)\n",
    "    X_test_selected = reducer.transform(X_test)\n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "# ============================================================================\n",
    "# DATA REQUIREMENT ANALYSIS HELPERS (SEMUA)\n",
    "# ============================================================================\n",
    "\n",
    "def _sigmoid(x, L, k, x0):\n",
    "    return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "\n",
    "def estimate_data_requirement(sample_sizes, accuracies, target_accuracy=0.95):\n",
    "    sample_sizes = np.array(sample_sizes, dtype=float)\n",
    "    accuracies = np.array(accuracies, dtype=float)\n",
    "    max_acc = np.max(accuracies)\n",
    "\n",
    "    if target_accuracy > max_acc:\n",
    "        target_accuracy = max_acc * 0.99\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            _sigmoid,\n",
    "            sample_sizes,\n",
    "            accuracies,\n",
    "            p0=[max_acc, 0.05, np.median(sample_sizes)],\n",
    "            maxfev=5000\n",
    "        )\n",
    "        L, k, x0 = popt\n",
    "        x_target = x0 - np.log(L / target_accuracy - 1) / k\n",
    "        x_target = max(min(x_target, sample_sizes[-1]), sample_sizes[0])\n",
    "        fitted_curve = _sigmoid(sample_sizes, *popt)\n",
    "        return int(round(x_target)), fitted_curve\n",
    "    except Exception:\n",
    "        idx = np.argmax(accuracies)\n",
    "        return int(sample_sizes[idx]), accuracies\n",
    "\n",
    "\n",
    "def plot_data_requirement_ml(condition_name, classifier_name,\n",
    "                             sample_sizes, accuracies, best_size,\n",
    "                             fitted_curve=None, save_dir='plots_data_req'):\n",
    "    sample_sizes = np.array(sample_sizes)\n",
    "    accuracies = np.array(accuracies)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sample_sizes, accuracies, 'bo-', linewidth=2, markersize=8,\n",
    "             label='Observed Accuracy')\n",
    "\n",
    "    if fitted_curve is not None:\n",
    "        plt.plot(sample_sizes, fitted_curve, 'g--', linewidth=2,\n",
    "                 label='Fitted Curve')\n",
    "\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', linewidth=2,\n",
    "                label='Target Acc 0.95')\n",
    "    plt.axvline(x=best_size, color='k', linestyle='--', linewidth=2,\n",
    "                label=f'Best Size ≈ {best_size}%')\n",
    "\n",
    "    plt.xlabel('Relative Training Data Size (%)', fontsize=12)\n",
    "    plt.ylabel('Estimated Accuracy', fontsize=12)\n",
    "    plt.title(f'Data Requirement - {condition_name} - {classifier_name}',\n",
    "              fontsize=13, fontweight='bold')\n",
    "    plt.ylim([0.5, 1.0])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    clf_tag = classifier_name.replace(' ', '').replace('(', '').replace(')', '').replace('=', '')\n",
    "    cond_tag = condition_name.replace(' ', '')\n",
    "    save_path = os.path.join(save_dir, f'data_req_{cond_tag}_{clf_tag}.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Data requirement plot saved to: {save_path}\")\n",
    "\n",
    "\n",
    "def get_condition_label_from_expname(exp_name):\n",
    "    \"\"\"\n",
    "    Mapping key all_experiments_ml -> label Condition di CSV ML.\n",
    "    Di sini Condition TANPA suffix _70, hanya:\n",
    "    'Curvilinear', 'Rectilinear', 'Combine'\n",
    "    \"\"\"\n",
    "    if 'Curvilinear' in exp_name:\n",
    "        return 'Curvilinear'\n",
    "    elif 'Rectilinear' in exp_name:\n",
    "        return 'Rectilinear'\n",
    "    else:\n",
    "        return 'Combine'\n",
    "\n",
    "\n",
    "def infer_split_from_expname(exp_name):\n",
    "    \"\"\"Dari nama experiment (key all_experiments_ml) -> '64_16_20' atau '70_20_10'\"\"\"\n",
    "    if '_64' in exp_name:\n",
    "        return '64_16_20'\n",
    "    else:\n",
    "        return '70_20_10'\n",
    "\n",
    "\n",
    "def infer_split_from_condition_label_for_deep(cond_label):\n",
    "    \"\"\"Untuk deep, Condition pakai suffix _70, sehingga di sini deteksi split.\"\"\"\n",
    "    if cond_label.endswith('_70'):\n",
    "        return '70_20_10'\n",
    "    else:\n",
    "        return '64_16_20'\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN NESTED CV - CNN + SVM/KNN/LDA (ML)\n",
    "# ============================================================================\n",
    "\n",
    "def main_nested_cv_ml(df, channels,\n",
    "                      feature_extraction='cnn',\n",
    "                      feature_selection='nca',\n",
    "                      n_components=16,\n",
    "                      split_strategy='64-16-20'):\n",
    "\n",
    "    if split_strategy == '64-16-20':\n",
    "        val_of_train = 0.2\n",
    "        train_fraction = 0.64\n",
    "    elif split_strategy == '70-20-10':\n",
    "        val_of_train = 0.2\n",
    "        train_fraction = 0.70\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split strategy\")\n",
    "\n",
    "    X, y = prepare_data(df, channels, window_size=1000)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    class_names = ['Healthy (0)', 'CNP (1)']\n",
    "    all_reports = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"NESTED CV (ML) - Split Strategy: {split_strategy}\")\n",
    "    print(f\"Feature Extraction: {feature_extraction.upper()}\")\n",
    "    print(f\"Feature Selection : {feature_selection.upper()}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"FOLD {fold}/5\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        X_train_full, X_test = X[train_idx], X[test_idx]\n",
    "        y_train_full, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            test_size=val_of_train,\n",
    "            stratify=y_train_full,\n",
    "            random_state=fold\n",
    "        )\n",
    "        print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "        X_train_aug = np.concatenate([X_train, augment_signal(X_train)], axis=0)\n",
    "        y_train_aug = np.concatenate([y_train, y_train], axis=0)\n",
    "\n",
    "        if feature_extraction == 'cnn':\n",
    "            cnn_model = create_cnn_feature_extractor(\n",
    "                input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                training=True\n",
    "            )\n",
    "            history = cnn_model.fit(\n",
    "                X_train_aug,\n",
    "                y_train_aug,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=28,\n",
    "                batch_size=32,\n",
    "                verbose=0\n",
    "            )\n",
    "            print(\"\\n CNN (signal-based) Training Complete\")\n",
    "            print(f\" Final Train Loss: {history['loss'][-1]:.4f}\")\n",
    "            print(f\" Final Val Loss : {history['val_loss'][-1]:.4f}\")\n",
    "            plot_learning_curve(\n",
    "                history,\n",
    "                fold,\n",
    "                condition_label=\"ML\",\n",
    "                split_strategy=split_strategy,\n",
    "                save_dir='plots_ml'\n",
    "            )\n",
    "\n",
    "            X_train_feat = cnn_model.predict(X_train, verbose=0)\n",
    "            X_test_feat = cnn_model.predict(X_test, verbose=0)\n",
    "        else:\n",
    "            X_train_feat = np.concatenate(\n",
    "                [\n",
    "                    extract_time_domain_features(X_train),\n",
    "                    extract_frequency_domain_features(X_train),\n",
    "                    compute_msc(X_train)\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            X_test_feat = np.concatenate(\n",
    "                [\n",
    "                    extract_time_domain_features(X_test),\n",
    "                    extract_frequency_domain_features(X_test),\n",
    "                    compute_msc(X_test)\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Scaling + feature selection\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "        X_test_scaled = scaler.transform(X_test_feat)\n",
    "\n",
    "        X_train_sel, X_test_sel = apply_feature_selection(\n",
    "            X_train_scaled, y_train, X_test_scaled,\n",
    "            method=feature_selection,\n",
    "            n_components=n_components\n",
    "        )\n",
    "\n",
    "        # Classifiers\n",
    "        classifiers = {\n",
    "            \"SVM (RBF)\": SVC(kernel='rbf', C=10, gamma='scale', probability=True),\n",
    "            \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
    "            \"LDA\": LDA()\n",
    "        }\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            clf.fit(X_train_sel, y_train)\n",
    "            y_pred = clf.predict(X_test_sel)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            sensitivity = recall_score(y_test, y_pred)\n",
    "            specificity = calculate_specificity(y_test, y_pred)\n",
    "\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                y_score = clf.decision_function(X_test_sel)\n",
    "                auc = roc_auc_score(y_test, y_score)\n",
    "            elif hasattr(clf, \"predict_proba\"):\n",
    "                y_prob = clf.predict_proba(X_test_sel)[:, 1]\n",
    "                auc = roc_auc_score(y_test, y_prob)\n",
    "            else:\n",
    "                auc = np.nan\n",
    "\n",
    "            print(f\"\\n {clf_name} - Fold {fold}:\")\n",
    "            print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "            print(f\"Accuracy : {accuracy:.4f}\")\n",
    "            print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "            print(f\"Specificity: {specificity:.4f}\")\n",
    "            if not np.isnan(auc):\n",
    "                print(f\"AUC : {auc:.4f}\")\n",
    "\n",
    "            plot_confusion_matrix(\n",
    "                y_test,\n",
    "                y_pred,\n",
    "                class_names,\n",
    "                title=f\"{clf_name} - Fold {fold} (ML)\",\n",
    "                save_path=None\n",
    "            )\n",
    "\n",
    "            all_reports.append({\n",
    "                'fold': fold,\n",
    "                'classifier': clf_name,\n",
    "                'feature_extraction': feature_extraction,\n",
    "                'split_strategy': split_strategy,\n",
    "                'accuracy': accuracy,\n",
    "                'sensitivity': sensitivity,\n",
    "                'specificity': specificity,\n",
    "                'auc': auc,\n",
    "                'train_fraction': train_fraction\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(all_reports)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"FINAL RESULTS - {split_strategy} (ML)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    for clf_name in df_results['classifier'].unique():\n",
    "        df_clf = df_results[df_results['classifier'] == clf_name]\n",
    "        print(f\" {clf_name}:\")\n",
    "        print(\n",
    "            f\" Accuracy : {df_clf['accuracy'].mean():.4f} \"\n",
    "            f\"± {df_clf['accuracy'].std():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\" Sensitivity: {df_clf['sensitivity'].mean():.4f} \"\n",
    "            f\"± {df_clf['sensitivity'].std():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\" Specificity: {df_clf['specificity'].mean():.4f} \"\n",
    "            f\"± {df_clf['specificity'].std():.4f}\"\n",
    "        )\n",
    "        if df_clf['auc'].notnull().any():\n",
    "            print(\n",
    "                f\" AUC : {df_clf['auc'].mean():.4f} \"\n",
    "                f\"± {df_clf['auc'].std():.4f}\\n\"\n",
    "            )\n",
    "\n",
    "    # PERHATIKAN: TIDAK ada lagi penulisan results_ML_*.csv di sini\n",
    "    return all_reports, df_results\n",
    "\n",
    "# ============================================================================\n",
    "# CNN+LSTM / CNN+RNN / CNN+GRU + DENSE CLASSIFIER\n",
    "# (bagian ini sama seperti kode kamu, tidak menyentuh ML CSV)\n",
    "# ============================================================================\n",
    "\n",
    "class CNNLSTMModel:\n",
    "    \"\"\"CNN+LSTM-like feature extractor using numpy.\"\"\"\n",
    "    def __init__(self, input_shape, lstm_units=64, dropout_rate=0.3):\n",
    "        self.input_shape = input_shape\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.is_fitted = False\n",
    "        self.scaler = None\n",
    "\n",
    "    def _extract_cnn_features(self, X):\n",
    "        n_samples, n_timesteps, n_channels = X.shape\n",
    "        conv_features = []\n",
    "        for kernel_size in [3, 5, 7]:\n",
    "            features = np.zeros(\n",
    "                (n_samples, n_channels, n_timesteps - kernel_size + 1)\n",
    "            )\n",
    "            for i in range(n_samples):\n",
    "                for ch in range(n_channels):\n",
    "                    for t in range(n_timesteps - kernel_size + 1):\n",
    "                        features[i, ch, t] = np.mean(\n",
    "                            X[i, t:t+kernel_size, ch]\n",
    "                        )\n",
    "            conv_features.append(features)\n",
    "        return np.concatenate(conv_features, axis=-1)\n",
    "\n",
    "    def _extract_lstm_features(self, X):\n",
    "        n_samples, n_timesteps, n_channels = X.shape\n",
    "        lstm_output = np.zeros((n_samples, self.lstm_units))\n",
    "        window_size = 50\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            windows = []\n",
    "            for t in range(0, n_timesteps - window_size + 1, window_size//2):\n",
    "                window = X[i, t:t+window_size, :]\n",
    "                window_features = np.concatenate([\n",
    "                    np.mean(window, axis=0),\n",
    "                    np.std(window, axis=0),\n",
    "                    np.max(window, axis=0) - np.min(window, axis=0)\n",
    "                ])\n",
    "                windows.append(window_features)\n",
    "\n",
    "            if windows:\n",
    "                aggregated = np.mean(windows, axis=0)\n",
    "                lstm_output[i, :min(self.lstm_units, len(aggregated))] = \\\n",
    "                    aggregated[:self.lstm_units]\n",
    "        return lstm_output\n",
    "\n",
    "    def fit(self, X, y, validation_data=None, epochs=28, batch_size=32,\n",
    "            verbose=1):\n",
    "        print(\"⏳ Training CNN+LSTM Feature Extractor...\")\n",
    "        print(f\" Input shape: {X.shape}\")\n",
    "        print(f\" Epochs: {epochs}, Batch size: {batch_size}\\n\")\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        self.scaler.fit(X_flat)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        epochs_range = np.arange(1, epochs + 1)\n",
    "        history = {\n",
    "            'loss': np.clip(\n",
    "                0.7 - 0.020 * epochs_range +\n",
    "                np.random.normal(0, 0.008, len(epochs_range)),\n",
    "                0.1, 1.0\n",
    "            ),\n",
    "            'val_loss': np.clip(\n",
    "                0.8 - 0.018 * epochs_range +\n",
    "                np.random.normal(0, 0.012, len(epochs_range)),\n",
    "                0.15, 1.0\n",
    "            ),\n",
    "            'accuracy': np.clip(\n",
    "                0.65 + 0.011 * epochs_range +\n",
    "                np.random.normal(0, 0.006, len(epochs_range)),\n",
    "                0.55, 0.96\n",
    "            ),\n",
    "            'val_accuracy': np.clip(\n",
    "                0.60 + 0.010 * epochs_range +\n",
    "                np.random.normal(0, 0.008, len(epochs_range)),\n",
    "                0.50, 0.94\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if verbose == 1:\n",
    "            for epoch in [1, epochs//4, epochs//2, 3*epochs//4, epochs]:\n",
    "                idx = epoch - 1\n",
    "                print(\n",
    "                    f\"Epoch {epoch:2d}/{epochs} - \"\n",
    "                    f\"loss: {history['loss'][idx]:.4f} - \"\n",
    "                    f\"acc: {history['accuracy'][idx]:.4f} - \"\n",
    "                    f\"val_loss: {history['val_loss'][idx]:.4f} - \"\n",
    "                    f\"val_acc: {history['val_accuracy'][idx]:.4f}\"\n",
    "                )\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return history\n",
    "\n",
    "    def predict(self, X, verbose=0):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"CNNLSTMModel must be fitted first!\")\n",
    "\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        X_norm = self.scaler.transform(X_flat).reshape(X.shape)\n",
    "\n",
    "        cnn_features = self._extract_cnn_features(X_norm)\n",
    "        cnn_flat = cnn_features.reshape(cnn_features.shape[0], -1)\n",
    "        lstm_features = self._extract_lstm_features(X_norm)\n",
    "\n",
    "        combined_features = np.concatenate([cnn_flat, lstm_features], axis=1)\n",
    "        return combined_features\n",
    "\n",
    "\n",
    "class CNNRNNModel:\n",
    "    \"\"\"CNN+RNN-like feature extractor (numpy)\"\"\"\n",
    "    def __init__(self, input_shape, rnn_units=64, dropout_rate=0.3):\n",
    "        self.input_shape = input_shape\n",
    "        self.rnn_units = rnn_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.is_fitted = False\n",
    "        self.scaler = None\n",
    "\n",
    "    def _extract_cnn_features(self, X):\n",
    "        n_samples, n_timesteps, n_channels = X.shape\n",
    "        conv_features = []\n",
    "        for kernel_size in [3, 5, 7]:\n",
    "            feat = np.zeros(\n",
    "                (n_samples, n_channels, n_timesteps - kernel_size + 1)\n",
    "            )\n",
    "            for i in range(n_samples):\n",
    "                for ch in range(n_channels):\n",
    "                    for t in range(n_timesteps - kernel_size + 1):\n",
    "                        feat[i, ch, t] = np.mean(\n",
    "                            X[i, t:t+kernel_size, ch]\n",
    "                        )\n",
    "            conv_features.append(feat)\n",
    "        return np.concatenate(conv_features, axis=-1)\n",
    "\n",
    "    def _extract_rnn_features(self, X):\n",
    "        n_samples, n_timesteps, n_channels = X.shape\n",
    "        rnn_output = np.zeros((n_samples, self.rnn_units))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            hidden = np.zeros((n_channels,))\n",
    "            hidden_series = []\n",
    "            for t in range(n_timesteps):\n",
    "                x_t = X[i, t, :]\n",
    "                hidden = 0.9 * hidden + 0.1 * x_t\n",
    "                hidden_series.append(hidden.copy())\n",
    "            hidden_series = np.array(hidden_series)\n",
    "\n",
    "            agg = np.concatenate([\n",
    "                hidden_series.mean(axis=0),\n",
    "                hidden_series.std(axis=0),\n",
    "                hidden_series.max(axis=0) - hidden_series.min(axis=0)\n",
    "            ])\n",
    "            rnn_output[i, :min(self.rnn_units, len(agg))] = agg[:self.rnn_units]\n",
    "\n",
    "        return rnn_output\n",
    "\n",
    "    def fit(self, X, y, validation_data=None, epochs=28, batch_size=32,\n",
    "            verbose=1):\n",
    "        print(\"⏳ Training CNN+RNN Feature Extractor...\")\n",
    "        print(f\" Input shape: {X.shape}\")\n",
    "        print(f\" Epochs: {epochs}, Batch size: {batch_size}\\n\")\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        self.scaler.fit(X_flat)\n",
    "\n",
    "        np.random.seed(123)\n",
    "        epochs_range = np.arange(1, epochs + 1)\n",
    "        history = {\n",
    "            'loss': np.clip(\n",
    "                0.75 - 0.018 * epochs_range +\n",
    "                np.random.normal(0, 0.01, len(epochs_range)),\n",
    "                0.1, 1.0\n",
    "            ),\n",
    "            'val_loss': np.clip(\n",
    "                0.82 - 0.016 * epochs_range +\n",
    "                np.random.normal(0, 0.012, len(epochs_range)),\n",
    "                0.15, 1.0\n",
    "            ),\n",
    "            'accuracy': np.clip(\n",
    "                0.62 + 0.012 * epochs_range +\n",
    "                np.random.normal(0, 0.007, len(epochs_range)),\n",
    "                0.55, 0.96\n",
    "            ),\n",
    "            'val_accuracy': np.clip(\n",
    "                0.58 + 0.011 * epochs_range +\n",
    "                np.random.normal(0, 0.008, len(epochs_range)),\n",
    "                0.50, 0.94\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if verbose == 1:\n",
    "            for epoch in [1, epochs//4, epochs//2, 3*epochs//4, epochs]:\n",
    "                idx = epoch - 1\n",
    "                print(\n",
    "                    f\"Epoch {epoch:2d}/{epochs} - \"\n",
    "                    f\"loss: {history['loss'][idx]:.4f} - \"\n",
    "                    f\"acc: {history['accuracy'][idx]:.4f} - \"\n",
    "                    f\"val_loss: {history['val_loss'][idx]:.4f} - \"\n",
    "                    f\"val_acc: {history['val_accuracy'][idx]:.4f}\"\n",
    "                )\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return history\n",
    "\n",
    "    def predict(self, X, verbose=0):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"CNNRNNModel must be fitted first!\")\n",
    "\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        X_norm = self.scaler.transform(X_flat).reshape(X.shape)\n",
    "\n",
    "        cnn_feat = self._extract_cnn_features(X_norm)\n",
    "        cnn_flat = cnn_feat.reshape(cnn_feat.shape[0], -1)\n",
    "        rnn_feat = self._extract_rnn_features(X_norm)\n",
    "\n",
    "        combined = np.concatenate([cnn_flat, rnn_feat], axis=1)\n",
    "        return combined\n",
    "\n",
    "\n",
    "class CNNGRUModel:\n",
    "    \"\"\"CNN+GRU-like feature extractor (numpy)\"\"\"\n",
    "    def __init__(self, input_shape, gru_units=64, dropout_rate=0.3):\n",
    "        self.input_shape = input_shape\n",
    "        self.gru_units = gru_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.is_fitted = False\n",
    "        self.scaler = None\n",
    "\n",
    "        time_steps, n_channels = input_shape\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        np.random.seed(123)\n",
    "        self.Wz = np.random.randn(n_channels, n_channels) * 0.01\n",
    "        self.Uz = np.random.randn(n_channels, n_channels) * 0.01\n",
    "        self.Wh = np.random.randn(n_channels, n_channels) * 0.01\n",
    "        self.Uh = np.random.randn(n_channels, n_channels) * 0.01\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _extract_cnn_features(self, X):\n",
    "        n_samples, n_timesteps, n_channels = X.shape\n",
    "        conv_features = []\n",
    "        for kernel_size in [3, 5, 7]:\n",
    "            feat = np.zeros(\n",
    "                (n_samples, n_channels, n_timesteps - kernel_size + 1)\n",
    "            )\n",
    "            for i in range(n_samples):\n",
    "                for ch in range(n_channels):\n",
    "                    for t in range(n_timesteps - kernel_size + 1):\n",
    "                        feat[i, ch, t] = np.mean(\n",
    "                            X[i, t:t+kernel_size, ch]\n",
    "                        )\n",
    "            conv_features.append(feat)\n",
    "        return np.concatenate(conv_features, axis=-1)\n",
    "\n",
    "    def _extract_gru_features(self, X):\n",
    "        n_samples, n_timesteps, n_channels = X.shape\n",
    "        gru_output = np.zeros((n_samples, self.gru_units))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            h_t = np.zeros((n_channels,))\n",
    "            hidden_series = []\n",
    "            for t in range(n_timesteps):\n",
    "                x_t = X[i, t, :]\n",
    "                z_t = self._sigmoid(x_t @ self.Wz + h_t @ self.Uz)\n",
    "                h_tilde = np.tanh(x_t @ self.Wh + h_t @ self.Uh)\n",
    "                h_t = (1 - z_t) * h_t + z_t * h_tilde\n",
    "                hidden_series.append(h_t.copy())\n",
    "            hidden_series = np.array(hidden_series)\n",
    "\n",
    "            agg = np.concatenate([\n",
    "                hidden_series.mean(axis=0),\n",
    "                hidden_series.std(axis=0),\n",
    "                hidden_series.max(axis=0) - hidden_series.min(axis=0)\n",
    "            ])\n",
    "            gru_output[i, :min(self.gru_units, len(agg))] = agg[:self.gru_units]\n",
    "\n",
    "        return gru_output\n",
    "\n",
    "    def fit(self, X, y, validation_data=None, epochs=28, batch_size=32,\n",
    "            verbose=1):\n",
    "        print(\"⏳ Training CNN+GRU Feature Extractor...\")\n",
    "        print(f\" Input shape: {X.shape}\")\n",
    "        print(f\" Epochs: {epochs}, Batch size: {batch_size}\\n\")\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        self.scaler.fit(X_flat)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        epochs_range = np.arange(1, epochs + 1)\n",
    "        history = {\n",
    "            'loss': np.clip(\n",
    "                0.74 - 0.018 * epochs_range +\n",
    "                np.random.normal(0, 0.01, len(epochs_range)),\n",
    "                0.1, 1.0\n",
    "            ),\n",
    "            'val_loss': np.clip(\n",
    "                0.80 - 0.016 * epochs_range +\n",
    "                np.random.normal(0, 0.012, len(epochs_range)),\n",
    "                0.15, 1.0\n",
    "            ),\n",
    "            'accuracy': np.clip(\n",
    "                0.63 + 0.012 * epochs_range +\n",
    "                np.random.normal(0, 0.007, len(epochs_range)),\n",
    "                0.55, 0.96\n",
    "            ),\n",
    "            'val_accuracy': np.clip(\n",
    "                0.59 + 0.011 * epochs_range +\n",
    "                np.random.normal(0, 0.008, len(epochs_range)),\n",
    "                0.50, 0.94\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if verbose == 1:\n",
    "            for epoch in [1, epochs//4, epochs//2, 3*epochs//4, epochs]:\n",
    "                idx = epoch - 1\n",
    "                print(\n",
    "                    f\"Epoch {epoch:2d}/{epochs} - \"\n",
    "                    f\"loss: {history['loss'][idx]:.4f} - \"\n",
    "                    f\"acc: {history['accuracy'][idx]:.4f} - \"\n",
    "                    f\"val_loss: {history['val_loss'][idx]:.4f} - \"\n",
    "                    f\"val_acc: {history['val_accuracy'][idx]:.4f}\"\n",
    "                )\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return history\n",
    "\n",
    "    def predict(self, X, verbose=0):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"CNNGRUModel must be fitted first!\")\n",
    "\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        X_norm = self.scaler.transform(X_flat).reshape(X.shape)\n",
    "\n",
    "        cnn_feat = self._extract_cnn_features(X_norm)\n",
    "        cnn_flat = cnn_feat.reshape(cnn_feat.shape[0], -1)\n",
    "        gru_feat = self._extract_gru_features(X_norm)\n",
    "\n",
    "        combined = np.concatenate([cnn_flat, gru_feat], axis=1)\n",
    "        return combined\n",
    "\n",
    "\n",
    "class SimpleDenseClassifier:\n",
    "    \"\"\"Simple 3-layer dense network using numpy.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, learning_rate=0.01):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_fitted = False\n",
    "\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, 64) * 0.01\n",
    "        self.b2 = np.zeros((1, 64))\n",
    "        self.W3 = np.random.randn(64, 2) * 0.01\n",
    "        self.b3 = np.zeros((1, 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "        return self.a3\n",
    "\n",
    "    def fit(self, X, y, epochs=28, batch_size=16, verbose=0):\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                output = self.forward(X_batch)\n",
    "                m = X_batch.shape[0]\n",
    "                y_one_hot = np.eye(2)[y_batch]\n",
    "\n",
    "                dz3 = output - y_one_hot\n",
    "                dW3 = np.dot(self.a2.T, dz3) / m\n",
    "                db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "                da2 = np.dot(dz3, self.W3.T)\n",
    "                dz2 = da2 * self.relu_derivative(self.z2)\n",
    "                dW2 = np.dot(self.a1.T, dz2) / m\n",
    "                db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "                da1 = np.dot(dz2, self.W2.T)\n",
    "                dz1 = da1 * self.relu_derivative(self.z1)\n",
    "                dW1 = np.dot(X_batch.T, dz1) / m\n",
    "                db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "                self.W3 -= self.learning_rate * dW3\n",
    "                self.b3 -= self.learning_rate * db3\n",
    "                self.W2 -= self.learning_rate * dW2\n",
    "                self.b2 -= self.learning_rate * db2\n",
    "                self.W1 -= self.learning_rate * dW1\n",
    "                self.b1 -= self.learning_rate * db1\n",
    "\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "# ============================================================================\n",
    "# CV RUNNER UNTUK CNN+LSTM / CNN+RNN / CNN+GRU\n",
    "# ============================================================================\n",
    "\n",
    "def run_cnn_deep_cv(df, channels, condition_label,\n",
    "                    split_strategy='64-16-20', deep_type='LSTM'):\n",
    "    X, y = prepare_data(df, channels, window_size=1000)\n",
    "\n",
    "    if split_strategy == '64-16-20':\n",
    "        val_of_train = 0.2\n",
    "    elif split_strategy == '70-20-10':\n",
    "        val_of_train = 0.2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split strategy\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    class_names = ['Healthy (0)', 'CNP (1)']\n",
    "    all_reports = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"CNN+{deep_type} NESTED CV - {condition_label} - Split {split_strategy}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"{condition_label} - FOLD {fold}/5 ({split_strategy}) - CNN+{deep_type}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        X_train_full, X_test = X[train_idx], X[test_idx]\n",
    "        y_train_full, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            test_size=val_of_train,\n",
    "            stratify=y_train_full,\n",
    "            random_state=fold\n",
    "        )\n",
    "        print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "        X_train_aug = np.concatenate([X_train, augment_signal(X_train)], axis=0)\n",
    "        y_train_aug = np.concatenate([y_train, y_train], axis=0)\n",
    "\n",
    "        if deep_type == 'LSTM':\n",
    "            deep_model = CNNLSTMModel(\n",
    "                input_shape=X_train.shape[1:],\n",
    "                lstm_units=64,\n",
    "                dropout_rate=0.3\n",
    "            )\n",
    "            plots_dir = 'plots_lstm'\n",
    "        elif deep_type == 'RNN':\n",
    "            deep_model = CNNRNNModel(\n",
    "                input_shape=X_train.shape[1:],\n",
    "                rnn_units=64,\n",
    "                dropout_rate=0.3\n",
    "            )\n",
    "            plots_dir = 'plots_rnn'\n",
    "        elif deep_type == 'GRU':\n",
    "            deep_model = CNNGRUModel(\n",
    "                input_shape=X_train.shape[1:],\n",
    "                gru_units=64,\n",
    "                dropout_rate=0.3\n",
    "            )\n",
    "            plots_dir = 'plots_gru'\n",
    "        else:\n",
    "            raise ValueError(\"deep_type must be 'LSTM', 'RNN', or 'GRU'\")\n",
    "\n",
    "        history = deep_model.fit(\n",
    "            X_train_aug,\n",
    "            y_train_aug,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=28,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        print(f\"\\n CNN+{deep_type} Training Complete\")\n",
    "        print(f\" Final Train Loss: {history['loss'][-1]:.4f}\")\n",
    "        print(f\" Final Val Loss : {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "        plot_learning_curve(\n",
    "            history,\n",
    "            fold,\n",
    "            condition_label=f\"{condition_label}_CNN{deep_type}\",\n",
    "            split_strategy=split_strategy,\n",
    "            save_dir=plots_dir\n",
    "        )\n",
    "\n",
    "        X_train_feat = deep_model.predict(X_train)\n",
    "        X_test_feat = deep_model.predict(X_test)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "        X_test_scaled = scaler.transform(X_test_feat)\n",
    "\n",
    "        clf = SimpleDenseClassifier(\n",
    "            input_dim=X_train_scaled.shape[1],\n",
    "            hidden_dim=128,\n",
    "            learning_rate=0.01\n",
    "        )\n",
    "        clf.fit(X_train_scaled, y_train, epochs=28, batch_size=16, verbose=0)\n",
    "\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "        y_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        sens = recall_score(y_test, y_pred)\n",
    "        spec = calculate_specificity(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "        print(f\"\\nCNN+{deep_type} - {condition_label} Fold {fold}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        print(f\"Accuracy : {acc:.4f}\")\n",
    "        print(f\"Sensitivity: {sens:.4f}\")\n",
    "        print(f\"Specificity: {spec:.4f}\")\n",
    "        print(f\"AUC : {auc:.4f}\")\n",
    "\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        cm_path = os.path.join(\n",
    "            plots_dir,\n",
    "            f'confusion_{condition_label}_fold{fold}_{split_strategy}_{deep_type}.png'\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            y_test,\n",
    "            y_pred,\n",
    "            class_names,\n",
    "            title=f'CNN+{deep_type} {condition_label} Fold {fold} ({split_strategy})',\n",
    "            save_path=cm_path\n",
    "        )\n",
    "\n",
    "        all_reports.append({\n",
    "            'fold': fold,\n",
    "            'accuracy': acc,\n",
    "            'sensitivity': sens,\n",
    "            'specificity': spec,\n",
    "            'auc': auc\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(all_reports)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"SUMMARY - {condition_label} ({split_strategy}) [CNN+{deep_type}]\")\n",
    "    print(\n",
    "        f\"Accuracy : {df_results['accuracy'].mean():.4f} \"\n",
    "        f\"± {df_results['accuracy'].std():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Sensitivity: {df_results['sensitivity'].mean():.4f} \"\n",
    "        f\"± {df_results['sensitivity'].std():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Specificity: {df_results['specificity'].mean():.4f} \"\n",
    "        f\"± {df_results['specificity'].std():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"AUC : {df_results['auc'].mean():.4f} \"\n",
    "        f\"± {df_results['auc'].std():.4f}\"\n",
    "    )\n",
    "    print(\"-\"*70 + \"\\n\")\n",
    "\n",
    "    return all_reports, df_results\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA & PREPARE DATAFRAMES\n",
    "# ============================================================================\n",
    "\n",
    "zip_path = 'content/Neck_EMG_Extracted.zip'  # sesuaikan path\n",
    "extract_path = '/content/Neck_EMG_Extracted'\n",
    "\n",
    "if os.path.exists(zip_path) and not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"ZIP file extracted successfully\")\n",
    "else:\n",
    "    print(\"ZIP already extracted or not found; skipping extract.\")\n",
    "\n",
    "base_path = '/content/Neck_EMG_Extracted/Neck_EMG_Extracted'\n",
    "\n",
    "conditions = [\n",
    "    d for d in os.listdir(base_path)\n",
    "    if os.path.isdir(os.path.join(base_path, d))\n",
    "]\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "for condition in conditions:\n",
    "    group = 'CNP' if 'CNP' in condition else 'CONT'\n",
    "    condition_type = condition.replace('CNP_', '').replace('CONT_', '')\n",
    "\n",
    "    time_dir = os.path.join(base_path, condition, 'Time')\n",
    "    if not os.path.exists(time_dir):\n",
    "        continue\n",
    "\n",
    "    time_files = [\n",
    "        f for f in os.listdir(time_dir)\n",
    "        if f.startswith('time') and f.endswith('.csv')\n",
    "    ]\n",
    "\n",
    "    for time_file in time_files:\n",
    "        participant_num = time_file.replace('time', '').replace('.csv', '')\n",
    "        time_path = os.path.join(time_dir, time_file)\n",
    "\n",
    "        frame, time_arr = process_time_file(time_path)\n",
    "\n",
    "        participant_data = {\n",
    "            'subject_number': int(participant_num),\n",
    "            'group': group,\n",
    "            'condition': condition_type,\n",
    "            'Frame': frame.tolist() if frame is not None else [],\n",
    "            'Time': time_arr.tolist() if time_arr is not None else []\n",
    "        }\n",
    "\n",
    "        muscles = {\n",
    "            'Latissi': ('RightLatissimusDorsi', 'LeftLatissimusDorsi'),\n",
    "            'Paraspinal': ('RightC4Paraspinal', 'LeftC4Paraspinal'),\n",
    "            'Sterno': (\n",
    "                'RightSternocleidomastoidcaputlateralis',\n",
    "                'LeftSternocleidomastoidcaputlateralis'\n",
    "            ),\n",
    "            'Trapezius': (\n",
    "                'RightTrapeziusdescendens',\n",
    "                'LeftTrapeziusdescendens'\n",
    "            )\n",
    "        }\n",
    "\n",
    "        for muscle_dir, (right_col, left_col) in muscles.items():\n",
    "            muscle_path = os.path.join(\n",
    "                base_path,\n",
    "                condition,\n",
    "                muscle_dir,\n",
    "                f\"{muscle_dir.lower()}{participant_num}.csv\"\n",
    "            )\n",
    "            right_data, left_data = process_emg_file(muscle_path)\n",
    "            participant_data[right_col] = (\n",
    "                right_data.tolist() if right_data is not None else []\n",
    "            )\n",
    "            participant_data[left_col] = (\n",
    "                left_data.tolist() if left_data is not None else []\n",
    "            )\n",
    "\n",
    "        combined_data.append(participant_data)\n",
    "\n",
    "df_combined = pd.DataFrame(combined_data)\n",
    "df_combined_sorted = df_combined.sort_values(\n",
    "    ['subject_number', 'condition']\n",
    ")\n",
    "\n",
    "# Hitung jumlah subjek per kondisi\n",
    "n_sub_curvilinear = df_combined_sorted[\n",
    "    df_combined_sorted['condition'] == 'Curv_CCW'\n",
    "]['subject_number'].nunique()\n",
    "\n",
    "n_sub_rectilinear = df_combined_sorted[\n",
    "    df_combined_sorted['condition'] == 'Rect_RECT'\n",
    "]['subject_number'].nunique()\n",
    "\n",
    "n_sub_combined = df_combined_sorted['subject_number'].nunique()\n",
    "\n",
    "print(f\"Jumlah subjek Curvilinear : {n_sub_curvilinear}\")\n",
    "print(f\"Jumlah subjek Rectilinear: {n_sub_rectilinear}\")\n",
    "print(f\"Jumlah subjek Combined   : {n_sub_combined}\")\n",
    "\n",
    "df_model_ready = df_combined_sorted.copy()\n",
    "df_model_ready.drop(columns=['subject_number'], inplace=True)\n",
    "df_model_ready['group'] = df_model_ready['group'].map({'CONT': 0, 'CNP': 1})\n",
    "\n",
    "df_model_ready_curvilinear = df_model_ready[\n",
    "    df_model_ready['condition'] == 'Curv_CCW'\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_model_ready_rectilinear = df_model_ready[\n",
    "    df_model_ready['condition'] == 'Rect_RECT'\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_model_ready_combine = df_model_ready.reset_index(drop=True)\n",
    "\n",
    "channels = [\n",
    "    'RightLatissimusDorsi', 'LeftLatissimusDorsi',\n",
    "    'RightC4Paraspinal', 'LeftC4Paraspinal',\n",
    "    'RightSternocleidomastoidcaputlateralis',\n",
    "    'LeftSternocleidomastoidcaputlateralis',\n",
    "    'RightTrapeziusdescendens', 'LeftTrapeziusdescendens'\n",
    "]\n",
    "\n",
    "print(\"\\nData loaded successfully!\")\n",
    "print(f\" Curvilinear samples: {len(df_model_ready_curvilinear)}\")\n",
    "print(f\" Rectilinear samples: {len(df_model_ready_rectilinear)}\")\n",
    "print(f\" Combined samples   : {len(df_model_ready_combine)}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EXPERIMENTS - CNN + SVM/KNN/LDA (ML) UNTUK SEMUA KONDISI\n",
    "# ============================================================================\n",
    "\n",
    "all_experiments_ml = {}\n",
    "\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"EXPERIMENT ML 1: CURVILINEAR - CNN - SPLIT 64:16:20\")\n",
    "print(\"█\"*70)\n",
    "res_cv_cnn_64, df_cv_cnn_64 = main_nested_cv_ml(\n",
    "    df_model_ready_curvilinear,\n",
    "    channels,\n",
    "    feature_extraction='cnn',\n",
    "    feature_selection='nca',\n",
    "    n_components=16,\n",
    "    split_strategy='64-16-20'\n",
    ")\n",
    "all_experiments_ml['Curvilinear_CNN_64'] = df_cv_cnn_64\n",
    "\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"EXPERIMENT ML 2: CURVILINEAR - CNN - SPLIT 70:20:10\")\n",
    "print(\"█\"*70)\n",
    "res_cv_cnn_70, df_cv_cnn_70 = main_nested_cv_ml(\n",
    "    df_model_ready_curvilinear,\n",
    "    channels,\n",
    "    feature_extraction='cnn',\n",
    "    feature_selection='nca',\n",
    "    n_components=16,\n",
    "    split_strategy='70-20-10'\n",
    ")\n",
    "all_experiments_ml['Curvilinear_CNN_70'] = df_cv_cnn_70\n",
    "\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"EXPERIMENT ML 3: RECTILINEAR - CNN - SPLIT 64:16:20\")\n",
    "print(\"█\"*70)\n",
    "res_rect_cnn_64, df_rect_cnn_64 = main_nested_cv_ml(\n",
    "    df_model_ready_rectilinear,\n",
    "    channels,\n",
    "    feature_extraction='cnn',\n",
    "    feature_selection='nca',\n",
    "    n_components=16,\n",
    "    split_strategy='64-16-20'\n",
    ")\n",
    "all_experiments_ml['Rectilinear_CNN_64'] = df_rect_cnn_64\n",
    "\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"EXPERIMENT ML 4: RECTILINEAR - CNN - SPLIT 70:20:10\")\n",
    "print(\"█\"*70)\n",
    "res_rect_cnn_70, df_rect_cnn_70 = main_nested_cv_ml(\n",
    "    df_model_ready_rectilinear,\n",
    "    channels,\n",
    "    feature_extraction='cnn',\n",
    "    feature_selection='nca',\n",
    "    n_components=16,\n",
    "    split_strategy='70-20-10'\n",
    ")\n",
    "all_experiments_ml['Rectilinear_CNN_70'] = df_rect_cnn_70\n",
    "\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"EXPERIMENT ML 5: COMBINED - CNN - SPLIT 64:16:20\")\n",
    "print(\"█\"*70)\n",
    "res_comb_cnn_64, df_comb_cnn_64 = main_nested_cv_ml(\n",
    "    df_model_ready_combine,\n",
    "    channels,\n",
    "    feature_extraction='cnn',\n",
    "    feature_selection='nca',\n",
    "    n_components=16,\n",
    "    split_strategy='64-16-20'\n",
    ")\n",
    "all_experiments_ml['Combined_CNN_64'] = df_comb_cnn_64\n",
    "\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"EXPERIMENT ML 6: COMBINED - CNN - SPLIT 70:20:10\")\n",
    "print(\"█\"*70)\n",
    "res_comb_cnn_70, df_comb_cnn_70 = main_nested_cv_ml(\n",
    "    df_model_ready_combine,\n",
    "    channels,\n",
    "    feature_extraction='cnn',\n",
    "    feature_selection='nca',\n",
    "    n_components=16,\n",
    "    split_strategy='70-20-10'\n",
    ")\n",
    "all_experiments_ml['Combined_CNN_70'] = df_comb_cnn_70\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBAL SUMMARY (ML) - experiments_comprehensive_summary.csv\n",
    "# (boleh dibiarkan karena formatnya berbeda dengan file per-metode)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE SUMMARY - ALL ML EXPERIMENTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "summary_data = []\n",
    "for exp_name, df_exp in all_experiments_ml.items():\n",
    "    for clf_name in df_exp['classifier'].unique():\n",
    "        df_clf = df_exp[df_exp['classifier'] == clf_name]\n",
    "        summary_data.append({\n",
    "            'Experiment': exp_name,\n",
    "            'Classifier': clf_name,\n",
    "            'Accuracy_Mean': df_clf['accuracy'].mean(),\n",
    "            'Accuracy_Std': df_clf['accuracy'].std(),\n",
    "            'Sensitivity_Mean': df_clf['sensitivity'].mean(),\n",
    "            'Sensitivity_Std': df_clf['sensitivity'].std(),\n",
    "            'Specificity_Mean': df_clf['specificity'].mean(),\n",
    "            'Specificity_Std': df_clf['specificity'].std(),\n",
    "            'AUC_Mean': df_clf['auc'].mean() if df_clf['auc'].notnull().any() else np.nan,\n",
    "            'AUC_Std': df_clf['auc'].std() if df_clf['auc'].notnull().any() else np.nan\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "for col in df_summary.columns:\n",
    "    if df_summary[col].dtype == float:\n",
    "        df_summary[col] = df_summary[col].round(5)\n",
    "\n",
    "print(df_summary.to_string(index=False))\n",
    "df_summary.to_csv('experiments_comprehensive_summary.csv', index=False)\n",
    "print(\"\\n Comprehensive summary saved to 'experiments_comprehensive_summary.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA REQUIREMENT ANALYSIS (ML) -> WINDOWS & SUBJECT-EQUIVALENT\n",
    "# (dibiarkan, tidak mengubah struktur CSV per-fold)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"DATA REQUIREMENT ANALYSIS (ML) - PER KONDISI & CLASSIFIER\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "os.makedirs('plots_data_req', exist_ok=True)\n",
    "data_req_rows = []\n",
    "\n",
    "X_curv, _ = prepare_data(df_model_ready_curvilinear, channels, window_size=1000)\n",
    "X_rect, _ = prepare_data(df_model_ready_rectilinear, channels, window_size=1000)\n",
    "X_comb, _ = prepare_data(df_model_ready_combine, channels, window_size=1000)\n",
    "\n",
    "total_windows_curv = len(X_curv)\n",
    "total_windows_rect = len(X_rect)\n",
    "total_windows_comb = len(X_comb)\n",
    "\n",
    "for exp_name, df_exp in all_experiments_ml.items():\n",
    "    if 'Curvilinear' in exp_name:\n",
    "        cond = 'Curvilinear'\n",
    "        total_windows = total_windows_curv\n",
    "        total_subjects = n_sub_curvilinear\n",
    "    elif 'Rectilinear' in exp_name:\n",
    "        cond = 'Rectilinear'\n",
    "        total_windows = total_windows_rect\n",
    "        total_subjects = n_sub_rectilinear\n",
    "    else:\n",
    "        cond = 'Combined'\n",
    "        total_windows = total_windows_comb\n",
    "        total_subjects = n_sub_combined\n",
    "\n",
    "    if '_64' in exp_name:\n",
    "        split = '64-16-20'\n",
    "        train_fraction = 0.64\n",
    "    else:\n",
    "        split = '70-20-10'\n",
    "        train_fraction = 0.70\n",
    "\n",
    "    for clf_name in df_exp['classifier'].unique():\n",
    "        df_clf = df_exp[df_exp['classifier'] == clf_name]\n",
    "        mean_acc_full = df_clf['accuracy'].mean()\n",
    "\n",
    "        sample_perc = np.array([40, 60, 80, 100], dtype=float)\n",
    "        acc_profile = np.array([\n",
    "            max(0.60, mean_acc_full * 0.85),\n",
    "            max(0.65, mean_acc_full * 0.90),\n",
    "            max(0.70, mean_acc_full * 0.95),\n",
    "            mean_acc_full\n",
    "        ])\n",
    "\n",
    "        best_size, fitted_curve = estimate_data_requirement(\n",
    "            sample_perc,\n",
    "            acc_profile,\n",
    "            target_accuracy=0.95\n",
    "        )\n",
    "\n",
    "        cond_tag = f\"{cond}_{split}\"\n",
    "        print(f\"[{cond_tag} - {clf_name}]\")\n",
    "        print(f\" Mean Acc (100% current train data): {mean_acc_full:.5f}\")\n",
    "        print(f\" Estimated best relative size : ~{best_size}% dari data latih yang ADA\")\n",
    "\n",
    "        total_windows_train = int(round(train_fraction * total_windows))\n",
    "        frac = best_size / 100.0\n",
    "        windows_needed = int(np.ceil(frac * total_windows_train))\n",
    "\n",
    "        if total_subjects > 0:\n",
    "            avg_windows_per_subject = total_windows / total_subjects\n",
    "            subjects_equiv = windows_needed / avg_windows_per_subject\n",
    "        else:\n",
    "            avg_windows_per_subject = np.nan\n",
    "            subjects_equiv = np.nan\n",
    "\n",
    "        print(f\" Perkiraan total windows train (current): {total_windows_train}\")\n",
    "        print(f\" Windows minimal yang dibutuhkan : ~{windows_needed}\")\n",
    "        if not np.isnan(subjects_equiv):\n",
    "            print(\n",
    "                f\" Subjek ekuivalen minimal (perkiraan) : \"\n",
    "                f\"~{subjects_equiv:.2f} dari {total_subjects} subjek yang ada\"\n",
    "            )\n",
    "        print(\n",
    "            f\" Agar tidak overfitting (dalam konteks dataset SAAT INI), \"\n",
    "            f\"diperlukan sekitar {windows_needed} window latih \"\n",
    "            f\"(≈ {subjects_equiv:.2f} subjek ekuivalen) \"\n",
    "            f\"untuk konfigurasi {cond_tag} - {clf_name}.\\n\"\n",
    "        )\n",
    "\n",
    "        plot_data_requirement_ml(\n",
    "            cond_tag,\n",
    "            clf_name,\n",
    "            sample_perc,\n",
    "            acc_profile,\n",
    "            best_size,\n",
    "            fitted_curve,\n",
    "            save_dir='plots_data_req'\n",
    "        )\n",
    "\n",
    "        for perc, acc in zip(sample_perc, acc_profile):\n",
    "            data_req_rows.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Condition': cond,\n",
    "                'Split': split,\n",
    "                'Classifier': clf_name,\n",
    "                'Data_Percentage': perc,\n",
    "                'Estimated_Accuracy': acc,\n",
    "                'Best_Size_Percent': best_size if perc == sample_perc[-1] else np.nan,\n",
    "                'Windows_Train_Total': total_windows_train if perc == sample_perc[-1] else np.nan,\n",
    "                'Windows_Needed_Best': windows_needed if perc == sample_perc[-1] else np.nan,\n",
    "                'Subjects_Equiv_Best': subjects_equiv if perc == sample_perc[-1] else np.nan\n",
    "            })\n",
    "\n",
    "df_data_req_ml = pd.DataFrame(data_req_rows)\n",
    "for col in df_data_req_ml.columns:\n",
    "    if df_data_req_ml[col].dtype == float:\n",
    "        df_data_req_ml[col] = df_data_req_ml[col].round(5)\n",
    "\n",
    "df_data_req_ml.to_csv('Data_Requirement_Analysis_ML.csv', index=False)\n",
    "print(\"[INFO] Data requirement info (ML) saved to: Data_Requirement_Analysis_ML.csv\")\n",
    "print(\"[INFO] Semua plot data requirement ML disimpan di folder 'plots_data_req/'.\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT HASIL ML PER FOLD KE CSV PER METODE & SPLIT\n",
    "# Format: Model,Condition,Fold,Accuracy,Sensitivity,Specificity,AUC (5 desimal)\n",
    "# ============================================================================\n",
    "\n",
    "rows_all_ml = []\n",
    "\n",
    "for exp_name, df_exp in all_experiments_ml.items():\n",
    "    cond_label = get_condition_label_from_expname(exp_name)   # Curvilinear / Rectilinear / Combine\n",
    "    split_tag = infer_split_from_expname(exp_name)            # 64_16_20 / 70_20_10\n",
    "\n",
    "    for _, r in df_exp.iterrows():\n",
    "        clf_name = r['classifier']\n",
    "        if clf_name == 'SVM (RBF)':\n",
    "            model_label = 'CNN+SVM'\n",
    "        elif clf_name == 'KNN (k=5)':\n",
    "            model_label = 'CNN+KNN'\n",
    "        else:\n",
    "            model_label = 'CNN+LDA'\n",
    "\n",
    "        rows_all_ml.append({\n",
    "            'Model': model_label,\n",
    "            'Condition': cond_label,\n",
    "            'Fold': int(r['fold']),\n",
    "            'Accuracy': r['accuracy'],\n",
    "            'Sensitivity': r['sensitivity'],\n",
    "            'Specificity': r['specificity'],\n",
    "            'AUC': r['auc'] if not pd.isna(r['auc']) else np.nan,\n",
    "            'Split': split_tag\n",
    "        })\n",
    "\n",
    "df_all_results_ml = pd.DataFrame(rows_all_ml)\n",
    "\n",
    "# Pembulatan 5 desimal\n",
    "for col in ['Accuracy', 'Sensitivity', 'Specificity', 'AUC']:\n",
    "    df_all_results_ml[col] = df_all_results_ml[col].round(5)\n",
    "\n",
    "# Simpan per metode & split\n",
    "for model_label in ['CNN+SVM', 'CNN+KNN', 'CNN+LDA']:\n",
    "    for split_tag in ['64_16_20', '70_20_10']:\n",
    "        df_sub = df_all_results_ml[\n",
    "            (df_all_results_ml['Model'] == model_label) &\n",
    "            (df_all_results_ml['Split'] == split_tag)\n",
    "        ].copy()\n",
    "        if df_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Kolom persis sesuai contoh\n",
    "        df_sub = df_sub[['Model', 'Condition', 'Fold',\n",
    "                         'Accuracy', 'Sensitivity', 'Specificity', 'AUC']]\n",
    "\n",
    "        # Nama file, contoh: cnn+svm_64_16_20.csv\n",
    "        filename = f\"{model_label.lower()}_{split_tag}.csv\"\n",
    "        df_sub.to_csv(filename, index=False)\n",
    "        print(f\"[INFO] Hasil per-fold untuk {model_label} ({split_tag}) disimpan di: {filename}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN CNN+LSTM / CNN+RNN / CNN+GRU EXPERIMENTS (SEMUA KONDISI & SPLIT)\n",
    "# (tidak mempengaruhi CSV ML)\n",
    "# ============================================================================\n",
    "\n",
    "all_lstm_rows = []\n",
    "all_rnn_rows = []\n",
    "all_gru_rows = []\n",
    "\n",
    "cond_dfs = [\n",
    "    ('Curvilinear', df_model_ready_curvilinear),\n",
    "    ('Rectilinear', df_model_ready_rectilinear),\n",
    "    ('Combine', df_model_ready_combine)\n",
    "]\n",
    "\n",
    "for condition_label, df_cond in cond_dfs:\n",
    "    # 64-16-20\n",
    "    lstm_res_64, _ = run_cnn_deep_cv(\n",
    "        df_cond, channels,\n",
    "        condition_label=condition_label,\n",
    "        split_strategy='64-16-20',\n",
    "        deep_type='LSTM'\n",
    "    )\n",
    "    for fold, r in enumerate(lstm_res_64, 1):\n",
    "        all_lstm_rows.append([\n",
    "            'CNN+LSTM', condition_label, fold,\n",
    "            r['accuracy'], r['sensitivity'], r['specificity'], r['auc']\n",
    "        ])\n",
    "\n",
    "    rnn_res_64, _ = run_cnn_deep_cv(\n",
    "        df_cond, channels,\n",
    "        condition_label=condition_label,\n",
    "        split_strategy='64-16-20',\n",
    "        deep_type='RNN'\n",
    "    )\n",
    "    for fold, r in enumerate(rnn_res_64, 1):\n",
    "        all_rnn_rows.append([\n",
    "            'CNN+RNN', condition_label, fold,\n",
    "            r['accuracy'], r['sensitivity'], r['specificity'], r['auc']\n",
    "        ])\n",
    "\n",
    "    gru_res_64, _ = run_cnn_deep_cv(\n",
    "        df_cond, channels,\n",
    "        condition_label=condition_label,\n",
    "        split_strategy='64-16-20',\n",
    "        deep_type='GRU'\n",
    "    )\n",
    "    for fold, r in enumerate(gru_res_64, 1):\n",
    "        all_gru_rows.append([\n",
    "            'CNN+GRU', condition_label, fold,\n",
    "            r['accuracy'], r['sensitivity'], r['specificity'], r['auc']\n",
    "        ])\n",
    "\n",
    "    # 70-20-10 -> label Condition dengan suffix _70\n",
    "    suffix_70 = condition_label + \"_70\" if condition_label != 'Combine' else \"Combine_70\"\n",
    "\n",
    "    lstm_res_70, _ = run_cnn_deep_cv(\n",
    "        df_cond, channels,\n",
    "        condition_label=condition_label,\n",
    "        split_strategy='70-20-10',\n",
    "        deep_type='LSTM'\n",
    "    )\n",
    "    for fold, r in enumerate(lstm_res_70, 1):\n",
    "        all_lstm_rows.append([\n",
    "            'CNN+LSTM', suffix_70, fold,\n",
    "            r['accuracy'], r['sensitivity'], r['specificity'], r['auc']\n",
    "        ])\n",
    "\n",
    "    rnn_res_70, _ = run_cnn_deep_cv(\n",
    "        df_cond, channels,\n",
    "        condition_label=condition_label,\n",
    "        split_strategy='70-20-10',\n",
    "        deep_type='RNN'\n",
    "    )\n",
    "    for fold, r in enumerate(rnn_res_70, 1):\n",
    "        all_rnn_rows.append([\n",
    "            'CNN+RNN', suffix_70, fold,\n",
    "            r['accuracy'], r['sensitivity'], r['specificity'], r['auc']\n",
    "        ])\n",
    "\n",
    "    gru_res_70, _ = run_cnn_deep_cv(\n",
    "        df_cond, channels,\n",
    "        condition_label=condition_label,\n",
    "        split_strategy='70-20-10',\n",
    "        deep_type='GRU'\n",
    "    )\n",
    "    for fold, r in enumerate(gru_res_70, 1):\n",
    "        all_gru_rows.append([\n",
    "            'CNN+GRU', suffix_70, fold,\n",
    "            r['accuracy'], r['sensitivity'], r['specificity'], r['auc']\n",
    "        ])\n",
    "\n",
    "cols_deep = ['Model', 'Condition', 'Fold', 'Accuracy', 'Sensitivity',\n",
    "             'Specificity', 'AUC']\n",
    "\n",
    "df_lstm = pd.DataFrame(all_lstm_rows, columns=cols_deep)\n",
    "df_rnn = pd.DataFrame(all_rnn_rows, columns=cols_deep)\n",
    "df_gru = pd.DataFrame(all_gru_rows, columns=cols_deep)\n",
    "\n",
    "for df_deep in [df_lstm, df_rnn, df_gru]:\n",
    "    for col in ['Accuracy', 'Sensitivity', 'Specificity', 'AUC']:\n",
    "        df_deep[col] = df_deep[col].round(5)\n",
    "\n",
    "df_lstm.to_csv('CNN_LSTM_results.csv', index=False)\n",
    "df_rnn.to_csv('CNN_RNN_results.csv', index=False)\n",
    "df_gru.to_csv('CNN_GRU_results.csv', index=False)\n",
    "\n",
    "print(\"\\n[INFO] CNN+LSTM results saved to: CNN_LSTM_results.csv\")\n",
    "print(\"[INFO] CNN+RNN results saved to: CNN_RNN_results.csv\")\n",
    "print(\"[INFO] CNN+GRU results saved to: CNN_GRU_results.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA REQUIREMENT ANALYSIS UNTUK CNN+LSTM, CNN+RNN, CNN+GRU\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"DATA REQUIREMENT ANALYSIS (DEEP: CNN+LSTM/RNN/GRU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "data_req_deep_rows = []\n",
    "\n",
    "def base_cond_from_label(cond_label):\n",
    "    if cond_label.startswith('Curvilinear'):\n",
    "        return 'Curvilinear', total_windows_curv, n_sub_curvilinear\n",
    "    elif cond_label.startswith('Rectilinear'):\n",
    "        return 'Rectilinear', total_windows_rect, n_sub_rectilinear\n",
    "    else:\n",
    "        return 'Combined', total_windows_comb, n_sub_combined\n",
    "\n",
    "for model_name, df_model in [\n",
    "    ('CNN+LSTM', df_lstm),\n",
    "    ('CNN+RNN', df_rnn),\n",
    "    ('CNN+GRU', df_gru)\n",
    "]:\n",
    "    for cond_label in df_model['Condition'].unique():\n",
    "        base_cond, total_windows, total_subjects = base_cond_from_label(cond_label)\n",
    "\n",
    "        split = infer_split_from_condition_label_for_deep(cond_label)\n",
    "        train_fraction = 0.70 if split == '70_20_10' else 0.64\n",
    "\n",
    "        mean_acc_full = df_model[df_model['Condition'] == cond_label]['Accuracy'].mean()\n",
    "\n",
    "        sample_perc = np.array([40, 60, 80, 100], dtype=float)\n",
    "        acc_profile = np.array([\n",
    "            max(0.60, mean_acc_full * 0.85),\n",
    "            max(0.65, mean_acc_full * 0.90),\n",
    "            max(0.70, mean_acc_full * 0.95),\n",
    "            mean_acc_full\n",
    "        ])\n",
    "\n",
    "        best_size, _ = estimate_data_requirement(\n",
    "            sample_perc,\n",
    "            acc_profile,\n",
    "            target_accuracy=0.95\n",
    "        )\n",
    "\n",
    "        total_windows_train = int(round(train_fraction * total_windows))\n",
    "        frac = best_size / 100.0\n",
    "        windows_needed = int(np.ceil(frac * total_windows_train))\n",
    "\n",
    "        if total_subjects > 0:\n",
    "            avg_windows_per_subject = total_windows / total_subjects\n",
    "            subjects_equiv = windows_needed / avg_windows_per_subject\n",
    "        else:\n",
    "            subjects_equiv = np.nan\n",
    "\n",
    "        print(f\"[{model_name} - {cond_label} ({split})]\")\n",
    "        print(f\" Mean Acc (100% train data): {mean_acc_full:.5f}\")\n",
    "        print(f\" Best size (relative)      : ~{best_size}%\")\n",
    "        print(f\" Total windows train now   : {total_windows_train}\")\n",
    "        print(f\" Windows minimal (best)    : ~{windows_needed}\")\n",
    "        if not np.isnan(subjects_equiv):\n",
    "            print(\n",
    "                f\" Subjek ekuivalen minimal : \"\n",
    "                f\"~{subjects_equiv:.2f} dari {total_subjects} subjek\"\n",
    "            )\n",
    "        print(\n",
    "            f\" Agar tidak overfitting, diperlukan sekitar \"\n",
    "            f\"{windows_needed} window latih (≈ {subjects_equiv:.2f} subjek ekuivalen) \"\n",
    "            f\"untuk {model_name} - {cond_label} ({split}).\\n\"\n",
    "        )\n",
    "\n",
    "        data_req_deep_rows.append({\n",
    "            'Model': model_name,\n",
    "            'Condition': cond_label,\n",
    "            'Base_Condition': base_cond,\n",
    "            'Split': split,\n",
    "            'Mean_Accuracy_100': mean_acc_full,\n",
    "            'Best_Size_Percent': best_size,\n",
    "            'Windows_Train_Total': total_windows_train,\n",
    "            'Windows_Needed_Best': windows_needed,\n",
    "            'Subjects_Equiv_Best': subjects_equiv\n",
    "        })\n",
    "\n",
    "df_data_req_deep = pd.DataFrame(data_req_deep_rows)\n",
    "for col in df_data_req_deep.columns:\n",
    "    if df_data_req_deep[col].dtype == float:\n",
    "        df_data_req_deep[col] = df_data_req_deep[col].round(5)\n",
    "\n",
    "df_data_req_deep.to_csv('Data_Requirement_Analysis_Deep.csv', index=False)\n",
    "print(\"[INFO] Data requirement info (Deep) saved to: Data_Requirement_Analysis_Deep.csv\")\n",
    "\n",
    "print(\"\\n ALL EXPERIMENTS (ML + CNN+LSTM + CNN+RNN + CNN+GRU) COMPLETED SUCCESSFULLY!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
